{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Лемматизация-текста\" data-toc-modified-id=\"Лемматизация-текста-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Лемматизация текста</a></span></li><li><span><a href=\"#Векторизация\" data-toc-modified-id=\"Векторизация-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Векторизация</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#RandomForestClassifier\" data-toc-modified-id=\"RandomForestClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>RandomForestClassifier</a></span></li><li><span><a href=\"#DecisionTreeClassifier\" data-toc-modified-id=\"DecisionTreeClassifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>DecisionTreeClassifier</a></span></li></ul></li><li><span><a href=\"#Проверка-LogisticRegression-на-тестовой-выборке\" data-toc-modified-id=\"Проверка-LogisticRegression-на-тестовой-выборке-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Проверка LogisticRegression на тестовой выборке</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('toxic_comments.csv')\n",
    "data.info()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.834884437596301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = data['toxic'].value_counts()[0] / data['toxic'].value_counts()[1]\n",
    "balance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "\n",
    "Пропусков и дубликатов в данных не наблюдается. Баланс классов нарушен, негативных текстов примерно в девять раз меньше чем положительных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    clear_text = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    clear_text = clear_text.split()\n",
    "    return \" \".join(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                         clear_text\n",
       "0      0  explanation why the edits made under my userna...\n",
       "1      0  d aww he matches this background colour i m se...\n",
       "2      0  hey man i m really not trying to edit war it s...\n",
       "3      0  more i can t make any real suggestions on impr...\n",
       "4      0  you sir are my hero any chance you remember wh..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clear_text'] = data['text'].apply(lambda x: clear_text(x)) \n",
    "data = data.drop(['text'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hozin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hozin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hozin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def my_lemmatizer(text):  \n",
    "    # токенизация и определение части языка\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #лемма если нет тэга\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
    "        else:        \n",
    "            #лемма если тэг есть\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>lemmas_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                         clear_text  \\\n",
       "0      0  explanation why the edits made under my userna...   \n",
       "1      0  d aww he matches this background colour i m se...   \n",
       "2      0  hey man i m really not trying to edit war it s...   \n",
       "3      0  more i can t make any real suggestions on impr...   \n",
       "4      0  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                         lemmas_text  \n",
       "0  explanation why the edits make under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not try to edit war it s ju...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir be my hero any chance you remember wha...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmas_text'] = data['clear_text'].apply(lambda x: my_lemmatizer(x))   \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['lemmas_text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hozin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "swords = stopwords.words('english')\n",
    "stopwords_set = set(swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords_set, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обозначим признаки, разделим выборки на обучающую и тестовую\n",
    "features = corpus\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7549588007298853\n",
      "Параметры :  {'logistic__C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "LR_model = LogisticRegression(class_weight = 'balanced', solver ='lbfgs', random_state=12345) \n",
    "pipe = Pipeline(steps=[('tf_idf', count_tf_idf), ('logistic', LR_model)])\n",
    "\n",
    "parameters = {'logistic__C': np.linspace(0.0001, 10, 7)\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, scoring='f1', cv=3)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "print('F1: ', grid_search.best_score_)\n",
    "print('Параметры : ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Логистической регресии F1-мера равна 0.7549588007298853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.38865234189028774\n"
     ]
    }
   ],
   "source": [
    "RFClf_model = RandomForestClassifier(class_weight = 'balanced', random_state=12345)\n",
    "pipe = Pipeline(steps=[('tf_idf', count_tf_idf), ('RFClf', RFClf_model)])\n",
    "\n",
    "parameters = {'RFClf__max_depth': np.linspace(10, 20, 16),\n",
    "             'RFClf__n_estimators': range(40, 60, 80)}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, scoring= 'f1', cv= 3)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "print('F1: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика качества F1 равна 0.38865234189028774, что тоже ниже необходимого результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.5973504653466848\n"
     ]
    }
   ],
   "source": [
    "DTClf_model = DecisionTreeClassifier(class_weight = 'balanced', random_state=12345)\n",
    "pipe = Pipeline(steps=[('tf_idf', count_tf_idf), ('DTClf', DTClf_model)])\n",
    "\n",
    "parameters = {'DTClf__max_depth': np.linspace(10, 20, 16)}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, scoring= 'f1', cv= 3)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "print('F1: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика качества F1 равна  0.5973504653466848"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка LogisticRegression на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf.fit(features_train)\n",
    "\n",
    "tf_idf_features_train = count_tf_idf.transform(features_train)\n",
    "tf_idf_features_test = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на тестовой выборке у LogisticRegression: 0.76\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=10.0, class_weight = 'balanced', solver ='lbfgs', random_state=12345) \n",
    "\n",
    "model.fit(tf_idf_features_train, target_train)\n",
    "predictions = model.predict(tf_idf_features_test)\n",
    "\n",
    "print('F1 на тестовой выборке у LogisticRegression: {:.2f}'.format(f1_score(target_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы было выполнено следующее:\n",
    "\n",
    "- Изучены и подготовлены данные.\n",
    "- Сделана лемматизация и векторизация текста.\n",
    "- Данные поделены на обучающую и тестовую выборки.\n",
    "- Выбраны и обучены модели.\n",
    "\n",
    "В итоге лучшей моделью стала LogisticRegression с метрикой качества F1 равной 0.76 на тестовых данных."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 5024,
    "start_time": "2021-10-01T14:47:11.238Z"
   },
   {
    "duration": 3376,
    "start_time": "2021-10-01T14:47:43.629Z"
   },
   {
    "duration": 4964,
    "start_time": "2021-10-01T14:48:01.390Z"
   },
   {
    "duration": 20,
    "start_time": "2021-10-01T14:49:40.763Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-01T14:52:11.894Z"
   },
   {
    "duration": 610,
    "start_time": "2021-10-01T16:00:44.614Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-01T16:01:55.840Z"
   },
   {
    "duration": 239,
    "start_time": "2021-10-01T16:02:03.678Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-01T16:05:00.973Z"
   },
   {
    "duration": 1845,
    "start_time": "2021-10-01T16:06:14.771Z"
   },
   {
    "duration": 1893,
    "start_time": "2021-10-01T16:06:22.539Z"
   },
   {
    "duration": 298,
    "start_time": "2021-10-01T16:09:23.760Z"
   },
   {
    "duration": 1269,
    "start_time": "2021-10-01T16:14:35.038Z"
   },
   {
    "duration": 625,
    "start_time": "2021-10-01T16:14:36.309Z"
   },
   {
    "duration": 31,
    "start_time": "2021-10-01T16:14:36.936Z"
   },
   {
    "duration": 215,
    "start_time": "2021-10-01T16:14:36.969Z"
   },
   {
    "duration": 2,
    "start_time": "2021-10-01T16:14:37.186Z"
   },
   {
    "duration": 6428,
    "start_time": "2021-10-01T16:14:37.190Z"
   },
   {
    "duration": 1237,
    "start_time": "2021-10-01T16:17:17.160Z"
   },
   {
    "duration": 620,
    "start_time": "2021-10-01T16:17:18.399Z"
   },
   {
    "duration": 26,
    "start_time": "2021-10-01T16:17:19.021Z"
   },
   {
    "duration": 214,
    "start_time": "2021-10-01T16:17:19.049Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-01T16:17:19.265Z"
   },
   {
    "duration": 6724,
    "start_time": "2021-10-01T16:17:19.270Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-01T16:18:01.821Z"
   },
   {
    "duration": 7145,
    "start_time": "2021-10-01T16:18:04.454Z"
   },
   {
    "duration": 1293,
    "start_time": "2021-10-01T16:18:37.011Z"
   },
   {
    "duration": 625,
    "start_time": "2021-10-01T16:18:38.306Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-01T16:18:38.933Z"
   },
   {
    "duration": 216,
    "start_time": "2021-10-01T16:18:38.960Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-01T16:18:39.177Z"
   },
   {
    "duration": 6453,
    "start_time": "2021-10-01T16:18:39.182Z"
   },
   {
    "duration": 57101,
    "start_time": "2021-10-01T16:20:45.719Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-01T16:21:51.425Z"
   },
   {
    "duration": 482992,
    "start_time": "2021-10-01T16:21:55.911Z"
   },
   {
    "duration": 1388,
    "start_time": "2021-10-01T16:30:36.822Z"
   },
   {
    "duration": 618,
    "start_time": "2021-10-01T16:30:38.212Z"
   },
   {
    "duration": 29,
    "start_time": "2021-10-01T16:30:38.833Z"
   },
   {
    "duration": 222,
    "start_time": "2021-10-01T16:30:38.864Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-01T16:30:39.087Z"
   },
   {
    "duration": 262126,
    "start_time": "2021-10-01T16:30:39.096Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-01T16:44:14.812Z"
   },
   {
    "duration": 578,
    "start_time": "2021-10-01T16:44:27.305Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-01T16:45:23.840Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-01T16:45:30.588Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-01T16:45:54.653Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-01T16:49:09.008Z"
   },
   {
    "duration": 281,
    "start_time": "2021-10-01T16:50:15.167Z"
   },
   {
    "duration": 267,
    "start_time": "2021-10-01T16:50:26.525Z"
   },
   {
    "duration": 178,
    "start_time": "2021-10-01T16:55:10.528Z"
   },
   {
    "duration": 164,
    "start_time": "2021-10-01T16:55:17.956Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-01T16:55:47.225Z"
   },
   {
    "duration": 554,
    "start_time": "2021-10-01T16:55:51.354Z"
   },
   {
    "duration": 595,
    "start_time": "2021-10-01T16:56:13.953Z"
   },
   {
    "duration": 184,
    "start_time": "2021-10-01T18:12:02.924Z"
   },
   {
    "duration": 171,
    "start_time": "2021-10-01T18:12:13.189Z"
   },
   {
    "duration": 200,
    "start_time": "2021-10-01T18:12:58.819Z"
   },
   {
    "duration": 172,
    "start_time": "2021-10-01T18:13:10.341Z"
   },
   {
    "duration": 846,
    "start_time": "2021-10-01T18:13:27.298Z"
   },
   {
    "duration": 273,
    "start_time": "2021-10-01T18:13:59.423Z"
   },
   {
    "duration": 171,
    "start_time": "2021-10-01T18:15:33.896Z"
   },
   {
    "duration": 11,
    "start_time": "2021-10-01T18:16:06.629Z"
   },
   {
    "duration": 256,
    "start_time": "2021-10-01T18:16:10.677Z"
   },
   {
    "duration": 188,
    "start_time": "2021-10-01T18:16:21.360Z"
   },
   {
    "duration": 176,
    "start_time": "2021-10-01T18:16:38.569Z"
   },
   {
    "duration": 170,
    "start_time": "2021-10-01T18:16:48.235Z"
   },
   {
    "duration": 1671,
    "start_time": "2021-10-01T18:16:59.044Z"
   },
   {
    "duration": 656,
    "start_time": "2021-10-01T18:19:40.384Z"
   },
   {
    "duration": 1739,
    "start_time": "2021-10-01T18:20:06.338Z"
   },
   {
    "duration": 1732,
    "start_time": "2021-10-01T18:20:30.682Z"
   },
   {
    "duration": 11559,
    "start_time": "2021-10-01T18:20:47.198Z"
   },
   {
    "duration": 176,
    "start_time": "2021-10-01T18:21:38.709Z"
   },
   {
    "duration": 10943,
    "start_time": "2021-10-01T18:21:45.283Z"
   },
   {
    "duration": 2339,
    "start_time": "2021-10-01T18:22:34.426Z"
   },
   {
    "duration": 4419,
    "start_time": "2021-10-01T18:22:50.820Z"
   },
   {
    "duration": 7578,
    "start_time": "2021-10-01T18:24:22.391Z"
   },
   {
    "duration": 264,
    "start_time": "2021-10-01T18:25:45.380Z"
   },
   {
    "duration": 738,
    "start_time": "2021-10-01T18:26:39.693Z"
   },
   {
    "duration": 787,
    "start_time": "2021-10-01T18:27:54.604Z"
   },
   {
    "duration": 742,
    "start_time": "2021-10-01T18:28:06.184Z"
   },
   {
    "duration": 752,
    "start_time": "2021-10-01T18:28:12.044Z"
   },
   {
    "duration": 257,
    "start_time": "2021-10-01T18:29:24.799Z"
   },
   {
    "duration": 263,
    "start_time": "2021-10-01T18:30:15.097Z"
   },
   {
    "duration": 290,
    "start_time": "2021-10-01T18:30:31.010Z"
   },
   {
    "duration": 304,
    "start_time": "2021-10-01T18:30:54.411Z"
   },
   {
    "duration": 6859,
    "start_time": "2021-10-01T18:32:37.021Z"
   },
   {
    "duration": 273,
    "start_time": "2021-10-01T18:33:21.779Z"
   },
   {
    "duration": 261,
    "start_time": "2021-10-01T18:34:18.163Z"
   },
   {
    "duration": 300,
    "start_time": "2021-10-01T18:34:46.347Z"
   },
   {
    "duration": 182,
    "start_time": "2021-10-01T18:35:47.397Z"
   },
   {
    "duration": 579,
    "start_time": "2021-10-01T18:35:59.206Z"
   },
   {
    "duration": 260,
    "start_time": "2021-10-01T18:36:58.200Z"
   },
   {
    "duration": 545,
    "start_time": "2021-10-01T18:37:07.473Z"
   },
   {
    "duration": 550,
    "start_time": "2021-10-01T18:37:15.172Z"
   },
   {
    "duration": 551,
    "start_time": "2021-10-01T18:38:07.931Z"
   },
   {
    "duration": 913,
    "start_time": "2021-10-01T18:38:25.841Z"
   },
   {
    "duration": 744,
    "start_time": "2021-10-01T18:38:41.333Z"
   },
   {
    "duration": 787,
    "start_time": "2021-10-01T18:38:51.712Z"
   },
   {
    "duration": 770,
    "start_time": "2021-10-01T18:38:58.516Z"
   },
   {
    "duration": 750,
    "start_time": "2021-10-01T18:39:11.825Z"
   },
   {
    "duration": 748,
    "start_time": "2021-10-01T18:39:41.910Z"
   },
   {
    "duration": 812,
    "start_time": "2021-10-01T18:39:49.186Z"
   },
   {
    "duration": 4506,
    "start_time": "2021-10-01T18:40:04.448Z"
   },
   {
    "duration": 2298,
    "start_time": "2021-10-01T18:40:20.500Z"
   },
   {
    "duration": 3359,
    "start_time": "2021-10-01T18:40:38.687Z"
   },
   {
    "duration": 3331,
    "start_time": "2021-10-01T18:40:56.244Z"
   },
   {
    "duration": 3396,
    "start_time": "2021-10-01T18:41:08.931Z"
   },
   {
    "duration": 1448,
    "start_time": "2021-10-01T18:44:26.578Z"
   },
   {
    "duration": 272,
    "start_time": "2021-10-01T18:45:13.551Z"
   },
   {
    "duration": 562,
    "start_time": "2021-10-01T18:45:26.302Z"
   },
   {
    "duration": 539,
    "start_time": "2021-10-01T18:45:34.567Z"
   },
   {
    "duration": 755,
    "start_time": "2021-10-01T18:45:46.782Z"
   },
   {
    "duration": 268,
    "start_time": "2021-10-01T18:47:53.918Z"
   },
   {
    "duration": 1444,
    "start_time": "2021-10-01T18:47:59.534Z"
   },
   {
    "duration": 1113,
    "start_time": "2021-10-01T18:57:13.699Z"
   },
   {
    "duration": 632,
    "start_time": "2021-10-01T18:57:14.814Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-01T18:57:15.450Z"
   },
   {
    "duration": 321,
    "start_time": "2021-10-01T18:57:15.477Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-01T18:57:15.799Z"
   },
   {
    "duration": 269868,
    "start_time": "2021-10-01T18:57:15.808Z"
   },
   {
    "duration": 11,
    "start_time": "2021-10-01T19:01:45.677Z"
   },
   {
    "duration": 242,
    "start_time": "2021-10-01T19:01:45.690Z"
   },
   {
    "duration": 2,
    "start_time": "2021-10-01T19:01:45.935Z"
   },
   {
    "duration": 12,
    "start_time": "2021-10-01T19:01:45.939Z"
   },
   {
    "duration": 759,
    "start_time": "2021-10-01T19:01:45.952Z"
   },
   {
    "duration": 3468,
    "start_time": "2021-10-01T19:01:46.712Z"
   },
   {
    "duration": 378,
    "start_time": "2021-10-01T19:01:50.182Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-01T19:03:30.762Z"
   },
   {
    "duration": 1425,
    "start_time": "2021-10-01T19:03:35.149Z"
   },
   {
    "duration": 1587,
    "start_time": "2021-10-02T10:15:48.357Z"
   },
   {
    "duration": 1007,
    "start_time": "2021-10-02T10:15:49.946Z"
   },
   {
    "duration": 40,
    "start_time": "2021-10-02T10:15:50.955Z"
   },
   {
    "duration": 265,
    "start_time": "2021-10-02T10:15:54.622Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-02T10:17:15.344Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-02T10:17:42.030Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-02T10:18:17.447Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-02T10:21:47.826Z"
   },
   {
    "duration": 2260,
    "start_time": "2021-10-02T10:22:14.722Z"
   },
   {
    "duration": 1223,
    "start_time": "2021-10-02T10:23:25.966Z"
   },
   {
    "duration": 745,
    "start_time": "2021-10-02T10:23:27.191Z"
   },
   {
    "duration": 26,
    "start_time": "2021-10-02T10:23:27.938Z"
   },
   {
    "duration": 253,
    "start_time": "2021-10-02T10:23:27.966Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-02T10:23:28.220Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-02T10:23:28.228Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-02T10:23:39.517Z"
   },
   {
    "duration": 2182,
    "start_time": "2021-10-02T10:23:40.382Z"
   },
   {
    "duration": 1347,
    "start_time": "2021-10-02T10:26:56.021Z"
   },
   {
    "duration": 266,
    "start_time": "2021-10-02T10:31:52.654Z"
   },
   {
    "duration": 2811,
    "start_time": "2021-10-02T10:32:21.453Z"
   },
   {
    "duration": 1546,
    "start_time": "2021-10-02T10:32:37.785Z"
   },
   {
    "duration": 2025,
    "start_time": "2021-10-02T10:33:28.691Z"
   },
   {
    "duration": 3441,
    "start_time": "2021-10-02T10:34:21.725Z"
   },
   {
    "duration": 1600,
    "start_time": "2021-10-02T10:34:36.690Z"
   },
   {
    "duration": 286,
    "start_time": "2021-10-02T10:34:50.694Z"
   },
   {
    "duration": 355,
    "start_time": "2021-10-02T10:37:01.221Z"
   },
   {
    "duration": 261,
    "start_time": "2021-10-02T10:37:53.862Z"
   },
   {
    "duration": 2128,
    "start_time": "2021-10-02T10:38:07.114Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-02T10:40:57.113Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-02T10:41:54.127Z"
   },
   {
    "duration": 316,
    "start_time": "2021-10-02T10:41:59.333Z"
   },
   {
    "duration": 258,
    "start_time": "2021-10-02T10:42:41.292Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-02T10:43:05.181Z"
   },
   {
    "duration": 669,
    "start_time": "2021-10-02T10:43:05.189Z"
   },
   {
    "duration": 30,
    "start_time": "2021-10-02T10:43:05.860Z"
   },
   {
    "duration": 246,
    "start_time": "2021-10-02T10:43:05.893Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-02T10:43:06.141Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-02T10:43:06.151Z"
   },
   {
    "duration": 18,
    "start_time": "2021-10-02T10:43:06.162Z"
   },
   {
    "duration": 2204,
    "start_time": "2021-10-02T10:43:06.183Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-02T10:43:08.389Z"
   },
   {
    "duration": 533220,
    "start_time": "2021-10-02T10:43:13.119Z"
   },
   {
    "duration": 2174,
    "start_time": "2021-10-02T11:00:50.911Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-02T11:00:53.087Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-02T11:00:53.095Z"
   },
   {
    "duration": 304,
    "start_time": "2021-10-02T11:01:21.977Z"
   },
   {
    "duration": 1532,
    "start_time": "2021-10-02T11:01:47.911Z"
   },
   {
    "duration": 658,
    "start_time": "2021-10-02T11:01:49.445Z"
   },
   {
    "duration": 28,
    "start_time": "2021-10-02T11:01:50.105Z"
   },
   {
    "duration": 244,
    "start_time": "2021-10-02T11:01:50.135Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-02T11:01:50.381Z"
   },
   {
    "duration": 14,
    "start_time": "2021-10-02T11:01:50.390Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-02T11:01:50.406Z"
   },
   {
    "duration": 2118,
    "start_time": "2021-10-02T11:01:50.412Z"
   },
   {
    "duration": 309,
    "start_time": "2021-10-02T11:01:52.533Z"
   },
   {
    "duration": 514766,
    "start_time": "2021-10-02T11:01:52.845Z"
   },
   {
    "duration": 2263,
    "start_time": "2021-10-02T11:10:27.613Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-02T11:10:29.878Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-02T11:10:29.885Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
